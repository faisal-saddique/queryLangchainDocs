Examples
========

🚧 _Docs under construction_ 🚧

Below are some examples for inspecting and checking different chains.

[

📄️ Agent VectorDB Question Answering Benchmarking
--------------------------------------------------

Here we go over how to benchmark performance on a question answering task using an agent to route between multiple vectordatabases.

](/docs/modules/evaluation/examples/agent_vectordb_sota_pg)

[

📄️ Comparing Chain Outputs
---------------------------

Suppose you have two different prompts (or LLMs). How do you know which will generate "better" results?

](/docs/modules/evaluation/examples/comparisons)

[

📄️ Data Augmented Question Answering
-------------------------------------

This notebook uses some generic prompts/language models to evaluate an question answering system that uses other sources of data besides what is in the model. For example, this can be used to evaluate a question answering system over your proprietary data.

](/docs/modules/evaluation/examples/data_augmented_question_answering)

[

📄️ Evaluating an OpenAPI Chain
-------------------------------

This notebook goes over ways to semantically evaluate an OpenAPI Chain, which calls an endpoint defined by the OpenAPI specification using purely natural language.

](/docs/modules/evaluation/examples/openapi_eval)

[

📄️ Question Answering Benchmarking: Paul Graham Essay
------------------------------------------------------

Here we go over how to benchmark performance on a question answering task over a Paul Graham essay.

](/docs/modules/evaluation/examples/qa_benchmarking_pg)

[

📄️ Question Answering Benchmarking: State of the Union Address
---------------------------------------------------------------

Here we go over how to benchmark performance on a question answering task over a state of the union address.

](/docs/modules/evaluation/examples/qa_benchmarking_sota)

[

📄️ QA Generation
-----------------

This notebook shows how to use the QAGenerationChain to come up with question-answer pairs over a specific document.

](/docs/modules/evaluation/examples/qa_generation)

[

📄️ Question Answering
----------------------

This notebook covers how to evaluate generic question answering problems. This is a situation where you have an example containing a question and its corresponding ground truth answer, and you want to measure how well the language model does at answering those questions.

](/docs/modules/evaluation/examples/question_answering)

[

📄️ SQL Question Answering Benchmarking: Chinook
------------------------------------------------

Here we go over how to benchmark performance on a question answering task over a SQL database.

](/docs/modules/evaluation/examples/sql_qa_benchmarking_chinook)